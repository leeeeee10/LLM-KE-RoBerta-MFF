# LLM-KE-RoBerta-MFF

## é¡¹ç›®æ¦‚è¿°âœ¨
æœ¬é¡¹ç›® `LLM-KE-RoBerta-MFF` èšç„¦äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸å…³çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæ¶µç›–äº†æ¨¡å‹å¾®è°ƒã€çŸ¥è¯†å¢å¼ºæ£€ç´¢ã€æ–‡æœ¬åˆ†ç±»ç­‰å¤šä¸ªæ–¹é¢çš„ä»»åŠ¡ã€‚é€šè¿‡æ•´åˆä¸åŒçš„æŠ€æœ¯å’Œæ¨¡å‹ï¼Œæ—¨åœ¨æå‡è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½å’Œæ•ˆæœã€‚

## é¡¹ç›®ç»“æ„ğŸ“
```
LLM-KE-RoBerta-MFF/
â”œâ”€â”€ lora_rag_LLM/
â”‚   â”œâ”€â”€ main.py               # æ¨¡å‹å¾®è°ƒä¸»ç¨‹åº
â”‚   â”œâ”€â”€ rag.py                # çŸ¥è¯†å¢å¼ºæ£€ç´¢æ¨¡å—
â”‚   â”œâ”€â”€ DeepSeek-R1-Distill-Qwen-1.5B/  # æ¨¡å‹æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ datasets.jsonl        # æ•°æ®é›†æ–‡ä»¶
â”‚   â”œâ”€â”€ final_models/         # æœ€ç»ˆä¿å­˜çš„å…¨é‡æ¨¡å‹
â”‚   â””â”€â”€ saved_models/         # ä¿å­˜çš„LoRAæ¨¡å‹
â”œâ”€â”€ CSTA-Corpus/
â”‚   â”œâ”€â”€ keyword.txt           # å…³é”®è¯æ–‡æœ¬æ–‡ä»¶
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ class.txt         # ç±»åˆ«æ–‡æœ¬æ–‡ä»¶
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ bert.py               # Robertaæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ bertè‡ªé€‚åº”.py          # è‡ªé€‚åº”Robertaæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ bert_RCNN.py          # BERTä¸RCNNç»“åˆæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ bertæ—¶é—´æ³¨æ„åŠ›.py       # å¸¦æ—¶é—´æ³¨æ„åŠ›çš„BERTæ¨¡å‹å®ç°
â”‚   â””â”€â”€ bert_RNN.py           # BERTä¸RNNç»“åˆæ¨¡å‹å®ç°
â”œâ”€â”€ run.py                    # è¿è¡Œè„šæœ¬
â”œâ”€â”€ train_eval.py             # è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
â””â”€â”€ utils.py                  # å·¥å…·å‡½æ•°è„šæœ¬
```

## ä¸»è¦åŠŸèƒ½æ¨¡å—

### 1. LoRAå¾®è°ƒä¸çŸ¥è¯†å¢å¼ºæ£€ç´¢ï¼ˆ`lora_rag_LLM`ï¼‰ğŸ§ 
- **`main.py`**: è´Ÿè´£åŠ è½½æ¨¡å‹ã€å‡†å¤‡æ•°æ®é›†ã€è¿›è¡Œæ¨¡å‹å¾®è°ƒä»¥åŠä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ã€‚ä½¿ç”¨äº†LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼ŒåŒæ—¶æ”¯æŒæ¨¡å‹é‡åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- **`rag.py`**: å®ç°äº†çŸ¥è¯†å¢å¼ºæ£€ç´¢ï¼ˆRAGï¼‰åŠŸèƒ½ï¼Œé€šè¿‡TF-IDFå‘é‡ç©ºé—´æ¨¡å‹å’ŒåŸºäºé¢†åŸŸåç§°çš„æ£€ç´¢æ–¹æ³•ï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼Œå¹¶å°†å…¶èå…¥åˆ°åŸå§‹æç¤ºä¸­ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å›ç­”èƒ½åŠ›ã€‚

### 2. æ–‡æœ¬åˆ†ç±»ï¼ˆ`models` å’Œ `run.py`ï¼‰ğŸ“š
- **`models` ç›®å½•**: åŒ…å«äº†å¤šç§åŸºäºBERTçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹å®ç°ï¼Œå¦‚BERTã€BERTä¸RCNNç»“åˆã€BERTä¸RNNç»“åˆç­‰ã€‚è¿™äº›æ¨¡å‹é€šè¿‡ä¸åŒçš„æ¶æ„å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡æ–‡æœ¬åˆ†ç±»çš„æ€§èƒ½ã€‚
- **`run.py`**: æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¿è¡Œè„šæœ¬ï¼Œè´Ÿè´£åŠ è½½æ•°æ®é›†ã€åˆå§‹åŒ–æ¨¡å‹ã€è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚è¿è¡Œæ—¶å¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ï¼Œå¦‚ `Bert` æˆ– `ERNIE`ã€‚

```python
# coding: UTF-8
import time
import torch
import numpy as np
from train_eval import train, init_network
from importlib import import_module
import argparse
from utils import build_dataset, build_iterator, get_time_dif

parser = argparse.ArgumentParser(description='Chinese Text Classification')
parser.add_argument('--model', type=str, required=True, help='choose a model: Bert, ERNIE')
args = parser.parse_args()

if __name__ == '__main__':
    dataset = 'CSTA-Corpus'  # æ•°æ®é›†

    model_name = args.model  # bert
    x = import_module('models.' + model_name)
    config = x.Config(dataset)
    np.random.seed(1)
    torch.manual_seed(1)
    torch.cuda.manual_seed_all(1)
    torch.backends.cudnn.deterministic = True  # ä¿è¯æ¯æ¬¡ç»“æœä¸€æ ·

    start_time = time.time()
    print("Loading data...")
    train_data, dev_data, test_data = build_dataset(config)
    train_iter = build_iterator(train_data, config)
    dev_iter = build_iterator(dev_data, config)
    test_iter = build_iterator(test_data, config)
    time_dif = get_time_dif(start_time)
    print("Time usage:", time_dif)

    test_iter = build_iterator(test_data, config, return_contents=True)  # å¯ç”¨è¿”å›æ–‡æœ¬

    # train
    model = x.Model(config).to(config.device)
    train(config, model, train_iter, dev_iter, test_iter, train_data)
```

## ç¯å¢ƒè¦æ±‚ğŸ–¥ï¸
- Python 3.x
- PyTorch
- Transformers
- Datasets
- Scikit-learn
- Numpy

## å®‰è£…ä¾èµ–ğŸ’»
```bash
pip install torch transformers datasets scikit-learn numpy
```

## ä½¿ç”¨æ–¹æ³•

### æ¨¡å‹å¾®è°ƒ
1. ç¡®ä¿ `lora_rag_LLM/DeepSeek-R1-Distill-Qwen-1.5B` ç›®å½•ä¸‹åŒ…å«æ‰€éœ€çš„æ¨¡å‹æ–‡ä»¶ã€‚
2. è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼š
```bash
python lora_rag_LLM/main.py
```
å¾®è°ƒåçš„LoRAæ¨¡å‹å°†ä¿å­˜åˆ° `lora_rag_LLM/saved_models` ç›®å½•ï¼Œå…¨é‡æ¨¡å‹å°†ä¿å­˜åˆ° `lora_rag_LLM/final_models` ç›®å½•ã€‚

### çŸ¥è¯†å¢å¼ºæ£€ç´¢
å¯ä»¥åœ¨ä»£ç ä¸­è°ƒç”¨ `rag.py` ä¸­çš„ `KnowMoveRAG` ç±»è¿›è¡ŒçŸ¥è¯†æ£€ç´¢å’Œæç¤ºå¢å¼ºï¼š
```python
from lora_rag_LLM.rag import KnowMoveRAG

rag = KnowMoveRAG()
original_prompt = "ç”Ÿæˆæ™ºèƒ½åˆ¶é€ ä¸è£…å¤‡é¢†åŸŸçš„å…³é”®è¯"
augmented_prompt = rag.augment_prompt(original_prompt)
print(augmented_prompt)
```

### æ–‡æœ¬åˆ†ç±»
è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼š
```bash
python run.py --model bert
```
å…¶ä¸­ `--model` å‚æ•°å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ï¼Œå¦‚ `bert`ã€`ERNIE` ç­‰ã€‚

## æ³¨æ„äº‹é¡¹âš ï¸
- åœ¨è¿è¡Œæ¨¡å‹å¾®è°ƒä¹‹å‰ï¼Œè¯·ç¡®ä¿ `lora_rag_LLM/datasets.jsonl` æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–è€…å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ `main.py` ä¸­çš„æ•°æ®é›†åŠ è½½éƒ¨åˆ†ã€‚
- æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„æ•°æ®é›†è·¯å¾„å’Œæ¨¡å‹ä¿å­˜è·¯å¾„å¯ä»¥åœ¨ `models` ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶ä¸­è¿›è¡Œä¿®æ”¹ã€‚

## è´¡çŒ®ğŸ¤
æ¬¢è¿å¯¹æœ¬é¡¹ç›®è¿›è¡Œè´¡çŒ®ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæå‡ºé—®é¢˜ã€æäº¤ä»£ç ã€æ”¹è¿›æ–‡æ¡£ç­‰ã€‚è¯·éµå¾ªé¡¹ç›®çš„è´¡çŒ®æŒ‡å—è¿›è¡Œæ“ä½œã€‚

## è”ç³»æ–¹å¼ğŸ“§
å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡GitHub Issuesä¸æˆ‘ä»¬è”ç³»ã€‚
